app:
  name: vit-ray

llm:
  model_name: Nous-Capybara-34B # supported = [Nous-Capybara-34B, Qwen-32B, Mistral-7B, C4AI-35B]  
  time_consecutive_res: 0.5
  serve_config:
    model: /data/nous-34b # supported - [/data/nous-34b, /data/qwen-32b, /data/mistral-7b, /data/c4ai-35b]
    download_dir: null # download model dir 
    load_format: auto #safetensors  # format of model {auto, pt, dummy, safetensors}
    dtype: float16  # data type {auto, float32, float16, bfloat16}
    max_model_len: 16384 # max length of model
    worker_use_ray: false # use ray for worker
    engine_use_ray: false # use ray for engine
    # pipeline_parallel_size: 1  # size of pipeline parallel
    tensor_parallel_size: 4  # size of tensor parallel
    # gpu_memory_utilization: 0.9  # gpu memory utilization
    enforce_eager: true
    disable_custom_all_reduce: True
    # trust_remote_code: true # for cohere models comment it

emb:
  serve_config:
    model_name: BAAI/bge-large-en-v1.5

loggers:  
  file:
    dir: /app/logs/
  console:
    rich: true  
    
